{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['ab', 'bc', 'ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a', 2: 'b', 3: 'c', 0: '.'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'aa',\n",
       " 1: 'ab',\n",
       " 2: 'ac',\n",
       " 3: 'a.',\n",
       " 4: 'ba',\n",
       " 5: 'bb',\n",
       " 6: 'bc',\n",
       " 7: 'b.',\n",
       " 8: 'ca',\n",
       " 9: 'cb',\n",
       " 10: 'cc',\n",
       " 11: 'c.',\n",
       " 12: '.a',\n",
       " 13: '.b',\n",
       " 14: '.c',\n",
       " 15: '..'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bichars = [x+y for x in stoi.keys() for y in stoi.keys()]\n",
    "bstoi = {s:i for i, s in enumerate(bichars)}\n",
    "# del bstoi['..']\n",
    "bitos = {i:s for s, i in bstoi.items()}\n",
    "bitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([12,  1, 13,  6, 14,  8]), tensor([2, 0, 3, 0, 1, 0]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = bstoi[ch1+ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "\n",
    "xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=16).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((16, 4), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7877308130264282\n",
      "1.7718868255615234\n",
      "1.756158471107483\n",
      "1.740545630455017\n",
      "1.7250484228134155\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    xenc = F.one_hot(xs, num_classes=16).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "    loss = -probs[torch.arange(probs.shape[0]), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -0.1*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "bcccccccbcccccccccbcca.\n",
      ".\n",
      "bcbacccccccccbcccccccccccccccca.\n",
      "b.\n"
     ]
    }
   ],
   "source": [
    "# sampling from neural net\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 15\n",
    "    while True:\n",
    "\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=16).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternative approach\n",
    "I attempted to represent each two letter word in a trigram as a concatenation of one-hot encoding for single letter. For instance, consider an alphabet of four characters ['.', 'a', 'b', 'c'], where:\n",
    "\n",
    "- 'a' is represented as [0, 1, 0, 0]\n",
    "- 'b' is represented as [0, 0, 1, 0]\n",
    "- Then, the bigram 'ab' is represented as [0, 1, 0, 0, 0, 0, 1, 0]. \n",
    "\n",
    "The model seems to be working but the loss plateaus at 2.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=len(chars)+1).float()\n",
    "xenc\n",
    "reshaped_tensor = xenc.view(xenc.size(0), -1)\n",
    "reshaped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674, -0.2373, -0.0274, -1.1008],\n",
       "        [ 0.2859, -0.0296, -1.5471,  0.6049],\n",
       "        [ 0.0791,  0.9046, -0.4713,  0.7868],\n",
       "        [-0.3284, -0.4330,  1.3729,  2.9334],\n",
       "        [ 1.5618, -1.6261,  0.6772, -0.8404],\n",
       "        [ 0.9849, -0.1484, -1.4795,  0.4483],\n",
       "        [-0.0707,  2.4968,  2.4448, -0.6701],\n",
       "        [-1.2199,  0.3031, -1.0725,  0.7276]], requires_grad=True)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((8, 4), generator=g, requires_grad=True)\n",
    "W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5522, -0.3857, -1.5069, -0.6525],\n",
       "        [ 0.2152,  2.4671,  0.8977, -0.0652],\n",
       "        [ 1.4966,  2.2595,  2.4174, -1.7708],\n",
       "        [-1.1407,  1.2078, -1.5438,  1.5144],\n",
       "        [ 0.3475,  0.0659, -1.0999, -0.3732],\n",
       "        [ 0.6564, -0.5813, -0.1066,  3.3817]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_tensor @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "total_chars = len(chars) + 1 # one for '.' char\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "_xenc = F.one_hot(xs, num_classes=total_chars).float()\n",
    "xenc = _xenc.view(_xenc.size(0), -1)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((total_chars*2, 27), generator=g, requires_grad=True) # multiplied by 2 because we are appending hot encoding of two characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.237211227416992\n",
      "2.237190008163452\n",
      "2.2371702194213867\n",
      "2.2371511459350586\n",
      "2.2371327877044678\n",
      "2.2371153831481934\n",
      "2.2370986938476562\n",
      "2.2370829582214355\n",
      "2.237067699432373\n",
      "2.237053155899048\n",
      "2.237039089202881\n",
      "2.237025737762451\n",
      "2.2370128631591797\n",
      "2.2370004653930664\n",
      "2.2369883060455322\n",
      "2.2369771003723145\n",
      "2.236966133117676\n",
      "2.236955404281616\n",
      "2.2369449138641357\n",
      "2.2369351387023926\n"
     ]
    }
   ],
   "source": [
    "for k in range(1000):\n",
    "    logits = xenc @ W\n",
    "    probs = logits.exp()\n",
    "    normed_probs = probs / probs.sum(1, keepdim=True)\n",
    "    \n",
    "\n",
    "    # loss = - sum(y*log(y_hat)+(1-y)*log(y_hat))\n",
    "    # loss = - yenc@normed_probs.T.log()+(1-yenc)@normed_probs.T.log()\n",
    "    loss = -normed_probs[torch.arange(probs.shape[0]), ys].log().mean()  # for row 0, ys 13; row 1, ys 23, row 2, ys 4 and so on\n",
    "\n",
    "    if k % 50 == 0:\n",
    "        print(loss.item())\n",
    "    \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -50*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maska",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
